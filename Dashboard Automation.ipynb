{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard Automation with Python Scripts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Paramteres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One python scripts which updates the charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#General Library imports\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Updates\n",
    "Elements to be updated include:\n",
    "1. Total Count and Experiment Count on the top of the dashboard\n",
    "2. Data Point Count Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Following File Labels we set identifiers for station codes\n",
    "station_id=['GDVR_NRSP','GDVR_RJMN','YMNA_DELH','GNGA_VRNS','GNGA_KLKT','BGLR','GNGA_UN','GNGA_FRKA','GNGA_PYRJ','GNGA_NROR','GNGA_JGPR','HIND_GZ']\n",
    "html_repo='thoreau-backend/frontend/templates/frontend/'\n",
    "csv_repo='/Users/utkarshrai/thoreau-backend/frontend/static/CSVData/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369941\n",
      "337\n",
      "[69674, 133058, 72095, 40532, 15004, 5857, 5567, 258, 873, 5231, 8045, 0]\n"
     ]
    }
   ],
   "source": [
    "#Iterate through the file list and count rows in each\n",
    "from itertools import takewhile, repeat\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "files=glob.glob(\"/Users/utkarshrai/thoreau-backend/frontend/static/CSVData/*.csv\")\n",
    "d = {f: file_len(f) for f in files}\n",
    "print(sum(d.values()))\n",
    "print(len(d.keys()))\n",
    "l_dat=[]\n",
    "for i in station_id:\n",
    "    files=glob.glob(csv_repo+i+'*.csv')\n",
    "    d = {f: file_len(f) for f in files}\n",
    "    l_dat.append(sum(d.values()))\n",
    "print(l_dat)\n",
    "map_str='var dat1='+str(l_dat)+';\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read and write the cooked HTML\n",
    "with open(html_repo+'dashboard_map.html', 'r') as file:\n",
    "    data = file.readlines()\n",
    "data[21]=map_str    #Replace our string in file\n",
    "with open(html_repo+'dashboard_map.html', 'w') as file:\n",
    "    file.writelines(data)\n",
    "    \n",
    "#21st line in dashboard_map.html hosts our map entries hence the replace is made.\n",
    "#When updating any HTML file please take care of these line numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Plot Updates\n",
    "\n",
    "With time new months have to be added\n",
    "When within the 3 month period, the last column updates on every boat ride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping is difficult so we'll just write the whole regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var dat=[[21484, 52715, 17406, 21120, 6480, 2781, 6736, 2695, 1641, 0, 0, 0], [3872, 17220, 13459, 19617, 8146, 2830, 973, 1424, 1339, 794, 0, 0], [0, 35877, 0, 0, 4203, 7265, 7370, 4863, 2871, 2214, 3931, 3501], [0, 0, 0, 5619, 3204, 3926, 8658, 5846, 2617, 0, 4460, 6202], [0, 0, 0, 0, 7359, 0, 4385, 1752, 1508, 0, 0, 0], [0, 0, 3932, 0, 2481, 0, 873, 1317, 7345, 2396, 7573, 13661]];\n",
      "\n"
     ]
    }
   ],
   "source": [
    "files=glob.glob(csv_repo+\"*.csv\")\n",
    "station_bar=['RJMN','NRSP','DELH','VRNS','KLKT']\n",
    "l_dat=[]\n",
    "for i in station_bar:\n",
    "    l_temp=[]\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('201704' in f or '201705' in f or '201706' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('201707' in f or '201708' in f or '201709' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('201710' in f or '201711' in f or '201712' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('201801' in f or '201802' in f or '201803' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('201804' in f or '201805' in f or '201806' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('201807' in f or '201808' in f or '201809' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('201810' in f or '201811' in f or '201812' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('201901' in f or '201902' in f or '201903' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('201904' in f or '201905' in f or '201906' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('201907' in f or '201908' in f or '201909' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('201910' in f or '201911' in f or '201912' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    d = {f: file_len(f) for f in files if (i in f) and ('202001' in f or '202002' in f or '202003' in f) }\n",
    "    l_temp.append(sum(d.values()))\n",
    "    l_dat.append(l_temp)\n",
    "l_temp=[]\n",
    "d = {f: file_len(f) for f in files if any(i in f for ext in station_bar) and ('201704' in f or '201705' in f or '201706' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "d = {f: file_len(f) for f in files if ('NRSP' not in f and 'RJMN' not in f and 'DELH' not in f and 'VRNS' not in f and 'KLKT' not in f) and ('201707' in f or '201708' in f or '201709' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "d = {f: file_len(f) for f in files if ('NRSP' not in f and 'RJMN' not in f and 'DELH' not in f and 'VRNS' not in f and 'KLKT' not in f) and ('201710' in f or '201711' in f or '201712' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "d = {f: file_len(f) for f in files if ('NRSP' not in f and 'RJMN' not in f and 'DELH' not in f and 'VRNS' not in f and 'KLKT' not in f) and ('201801' in f or '201802' in f or '201803' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "d = {f: file_len(f) for f in files if ('NRSP' not in f and 'RJMN' not in f and 'DELH' not in f and 'VRNS' not in f and 'KLKT' not in f) and ('201804' in f or '201805' in f or '201806' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "d = {f: file_len(f) for f in files if ('NRSP' not in f and 'RJMN' not in f and 'DELH' not in f and 'VRNS' not in f and 'KLKT' not in f) and ('201807' in f or '201808' in f or '201809' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "d = {f: file_len(f) for f in files if ('NRSP' not in f and 'RJMN' not in f and 'DELH' not in f and 'VRNS' not in f and 'KLKT' not in f) and ('201810' in f or '201811' in f or '201812' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "d = {f: file_len(f) for f in files if ('NRSP' not in f and 'RJMN' not in f and 'DELH' not in f and 'VRNS' not in f and 'KLKT' not in f) and ('201901' in f or '201902' in f or '201903' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "d = {f: file_len(f) for f in files if ('NRSP' not in f and 'RJMN' not in f and 'DELH' not in f and 'VRNS' not in f and 'KLKT' not in f) and ('201904' in f or '201905' in f or '201906' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "d = {f: file_len(f) for f in files if ('NRSP' not in f and 'RJMN' not in f and 'DELH' not in f and 'VRNS' not in f and 'KLKT' not in f) and ('201907' in f or '201908' in f or '201909' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "d = {f: file_len(f) for f in files if ('NRSP' not in f and 'RJMN' not in f and 'DELH' not in f and 'VRNS' not in f and 'KLKT' not in f) and ('201910' in f or '201911' in f or '201912' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "d = {f: file_len(f) for f in files if ('NRSP' not in f and 'RJMN' not in f and 'DELH' not in f and 'VRNS' not in f and 'KLKT' not in f) and ('202001' in f or '202002' in f or '202003' in f) }\n",
    "l_temp.append(sum(d.values()))\n",
    "l_dat.append(l_temp)\n",
    "column_str='var dat='+str(l_dat)+';\\n'\n",
    "print(column_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(html_repo+'bar_plot.html', 'r') as file:\n",
    "    data = file.readlines()\n",
    "data[6]=column_str    #Replace our string in file\n",
    "with open(html_repo+'bar_plot.html', 'w') as file:\n",
    "    file.writelines(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes Creation\n",
    "\n",
    "Linearly we build dataframes for all our stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Narsapur new_df_NRSP\n",
    "l=glob.glob(\"/Users/utkarshrai/thoreau-backend/frontend/static/CSVData/GDVR_NRSP*.csv\")\n",
    "dfs=[]\n",
    "for filename in l:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['Date']=filename[68:76]\n",
    "    df['Time']=df['Time'][0]\n",
    "    dfs.append(df)\n",
    "\n",
    "new_df_NRSP = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "new_df_NRSP['Date'] =  pd.to_datetime(new_df_NRSP['Date']+' '+new_df_NRSP['Time'])\n",
    "new_df_NRSP=new_df_NRSP.groupby(new_df_NRSP['Date']).median()\n",
    "new_df_NRSP.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rajahmundry\n",
    "l=glob.glob(\"/Users/utkarshrai/thoreau-backend/frontend/static/CSVData/GDVR_RJMN*.csv\")\n",
    "dfs=[]\n",
    "for filename in l:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['Date']=filename[68:76]\n",
    "    df['Time']=df['Time'][0]\n",
    "    dfs.append(df)\n",
    "\n",
    "new_df_RJMN = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "count_RJMN=new_df_RJMN['Date'].count()\n",
    "new_df_RJMN['Date'] =  pd.to_datetime(new_df_RJMN['Date']+' '+new_df_RJMN['Time'])\n",
    "new_df_RJMN=new_df_RJMN.groupby(new_df_RJMN['Date']).median()\n",
    "new_df_RJMN.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in l:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['Date']=filename[68:76]\n",
    "    df['Time']=df['Time'][0]\n",
    "    dfs.append(df)\n",
    "\n",
    "new_df_YM_DL = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "new_df_YM_DL['Date'] =  pd.to_datetime(new_df_YM_DL['Date']+' '+new_df_YM_DL['Time'])\n",
    "new_df_YM_DL=new_df_YM_DL.groupby(new_df_YM_DL['Date']).median()\n",
    "new_df_YM_DL.reset_index(inplace=True)\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=glob.glob(\"/Users/utkarshrai/thoreau-backend/frontend/static/CSVData/GNGA_VRNS*.csv\")\n",
    "dfs=[]\n",
    "for filename in l:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['Date']=filename[68:76]\n",
    "    df['Time']=df['Time'][0]\n",
    "    dfs.append(df)\n",
    "\n",
    "new_df_VRNS = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "count_VRNS=new_df_VRNS['Date'].count()\n",
    "new_df_VRNS['Date'] =  pd.to_datetime(new_df_VRNS['Date']+' '+new_df_VRNS['Time'])\n",
    "new_df_VRNS=new_df_VRNS.groupby(new_df_VRNS['Date']).median()\n",
    "new_df_VRNS.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "l=glob.glob(\"/Users/utkarshrai/thoreau-backend/frontend/static/CSVData/GNGA_KLKT*.csv\")\n",
    "dfs=[]\n",
    "for filename in l:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['Date']=filename[68:76]\n",
    "    df['Time']=df['Time'][0]\n",
    "    dfs.append(df)\n",
    "\n",
    "new_df_KLKT = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "new_df_KLKT['Date'] =  pd.to_datetime(new_df_KLKT['Date']+' '+new_df_KLKT['Time'])\n",
    "new_df_KLKT=new_df_KLKT.groupby(new_df_KLKT['Date']).median()\n",
    "new_df_KLKT.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=glob.glob(\"/Users/utkarshrai/thoreau-backend/frontend/static/CSVData/GNGA_UN*.csv\")\n",
    "dfs=[]\n",
    "for filename in l:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['Date']=filename[68:76]\n",
    "    df['Time']=df['Time'][0]\n",
    "    dfs.append(df)\n",
    "\n",
    "new_df_KNPR = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "new_df_KNPR['Date'] =  pd.to_datetime(new_df_KNPR['Date']+' '+new_df_KNPR['Time'])\n",
    "new_df_KNPR=new_df_KNPR.groupby(new_df_KNPR['Date']).median()\n",
    "new_df_KNPR.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=glob.glob(\"/Users/utkarshrai/thoreau-backend/frontend/static/CSVData/GNGA_NROR*.csv\")\n",
    "dfs=[]\n",
    "for filename in l:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['Date']=filename[68:76]\n",
    "    df['Time']=df['Time'][0]\n",
    "    dfs.append(df)\n",
    "\n",
    "new_df_NR = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "new_df_NR['Date'] =  pd.to_datetime(new_df_NR['Date']+' '+new_df_NR['Time'])\n",
    "new_df_NR=new_df_NR.groupby(new_df_NR['Date']).median()\n",
    "new_df_NR.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=glob.glob(\"/Users/utkarshrai/thoreau-backend/frontend/static/CSVData/BGLR*.csv\")\n",
    "dfs=[]\n",
    "for filename in l:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['Date']=filename[68:76]\n",
    "    df['Time']=df['Time'][0]\n",
    "    dfs.append(df)\n",
    "\n",
    "new_df_BGLR = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "new_df_BGLR['Date'] =  pd.to_datetime(new_df_BGLR['Date']+' '+new_df_BGLR['Time'])\n",
    "new_df_BGLR=new_df_BGLR.groupby(new_df_BGLR['Date']).median()\n",
    "new_df_BGLR.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=glob.glob(\"/Users/utkarshrai/thoreau-backend/frontend/static/CSVData/GNGA_JGPR*.csv\")\n",
    "\n",
    "dfs=[]\n",
    "for filename in l[:3]:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['Date']=filename[68:76]\n",
    "    df['Time']=df['Time'][0]\n",
    "    dfs.append(df)\n",
    "\n",
    "new_df_JGPR = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "new_df_JGPR['Date'] =  pd.to_datetime(new_df_JGPR['Date']+' '+new_df_JGPR['Time'])\n",
    "new_df_JGPR=new_df_JGPR.groupby(new_df_JGPR['Date']).median()\n",
    "new_df_JGPR.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=glob.glob(\"/Users/utkarshrai/thoreau-backend/frontend/static/CSVData/GNGA_PYRJ*.csv\")\n",
    "dfs=[]\n",
    "for filename in l:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df['Date']=filename[68:76]\n",
    "    df['Time']=df['Time'][0]\n",
    "    dfs.append(df)\n",
    "\n",
    "new_df_PYRJ = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "new_df_PYRJ['Date'] =  pd.to_datetime(new_df_PYRJ['Date']+' '+new_df_PYRJ['Time'])\n",
    "new_df_PYRJ=new_df_PYRJ.groupby(new_df_PYRJ['Date']).median()\n",
    "new_df_PYRJ.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot Updates\n",
    "\n",
    "With new readings additions/substraction happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_1=[\"#7cb5ec\", \"#434348\", \"#90ed7d\", \"#f7a35c\", \"#8085e9\", \"#f15c80\", \"#e4d354\", \"#2b908f\", \"#f45b5b\", \"#91e8e1\"]\n",
    "newL=['pH ','Temp[C]','D.O.[ppm]','Turb[FNU]', 'EC[muS per cm]','Am[mV]','Ni[ppm]','CDOM [RFU]','CHL-a [RFU]','Tryptophan [RFU]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1='['\n",
    "for i in newL:\n",
    "    p=new_df_RJMN[i].dropna().describe()\n",
    "    s1+='[{x:1,low:'+str(p['min'])+\",q1: \"+str(p['25%'])+\",median: \"+str(p['50%'])+\",q3:\" +str(p['75%'])+\",high: \"+str(p['max'])+',color:\\''+color_1[0]+'\\'},'\n",
    "    p=new_df_NRSP[i].dropna().describe()\n",
    "    s1+='{x:2,low:'+str(p['min'])+\",q1: \"+str(p['25%'])+\",median: \"+str(p['50%'])+\",q3:\" +str(p['75%'])+\",high: \"+str(p['max'])+',color:\\''+color_1[1]+'\\'},'\n",
    "    p=new_df_YM_DL[i].dropna().describe()\n",
    "    s1+='{x:3,low:'+str(p['min'])+\",q1: \"+str(p['25%'])+\",median: \"+str(p['50%'])+\",q3:\" +str(p['75%'])+\",high: \"+str(p['max'])+',color:\\''+color_1[2]+'\\'},'\n",
    "    p=new_df_VRNS[i].dropna().describe()\n",
    "    s1+='{x:4,low:'+str(p['min'])+\",q1: \"+str(p['25%'])+\",median: \"+str(p['50%'])+\",q3:\" +str(p['75%'])+\",high: \"+str(p['max'])+',color:\\''+color_1[3]+'\\'},'\n",
    "    p=new_df_KLKT[i].dropna().describe()\n",
    "    s1+='{x:5,low:'+str(p['min'])+\",q1: \"+str(p['25%'])+\",median: \"+str(p['50%'])+\",q3:\" +str(p['75%'])+\",high: \"+str(p['max'])+',color:\\''+color_1[4]+'\\'},'\n",
    "    p=new_df_KNPR[i].dropna().describe()\n",
    "    s1+='{x:6,low:'+str(p['min'])+\",q1: \"+str(p['25%'])+\",median: \"+str(p['50%'])+\",q3:\" +str(p['75%'])+\",high: \"+str(p['max'])+',color:\\''+color_1[5]+'\\'},'\n",
    "    p=new_df_NR[i].dropna().describe()\n",
    "    s1+='{x:7,low:'+str(p['min'])+\",q1: \"+str(p['25%'])+\",median: \"+str(p['50%'])+\",q3:\" +str(p['75%'])+\",high: \"+str(p['max'])+',color:\\''+color_1[6]+'\\'},'\n",
    "    p=new_df_BGLR[i].dropna().describe()\n",
    "    s1+='{x:8,low:'+str(p['min'])+\",q1: \"+str(p['25%'])+\",median: \"+str(p['50%'])+\",q3:\" +str(p['75%'])+\",high: \"+str(p['max'])+',color:\\''+color_1[7]+'\\'},'\n",
    "    p=new_df_PYRJ[i].dropna().describe()\n",
    "    s1+='{x:9,low:'+str(p['min'])+\",q1: \"+str(p['25%'])+\",median: \"+str(p['50%'])+\",q3:\" +str(p['75%'])+\",high: \"+str(p['max'])+',color:\\''+color_1[8]+'\\'},'\n",
    "    p=new_df_JGPR[i].dropna().describe()\n",
    "    s1+='{x:10,low:'+str(p['min'])+\",q1: \"+str(p['25%'])+\",median: \"+str(p['50%'])+\",q3:\" +str(p['75%'])+\",high: \"+str(p['max'])+',color:\\''+color_1[9]+'\\'}],'\n",
    "s1=s1[:-1]\n",
    "s1=s1.replace('nan','null')\n",
    "s1+='];'\n",
    "s1='var dat1='+s1+';\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(html_repo+'box_plot_2.html', 'r') as file:\n",
    "    data = file.readlines()\n",
    "data[26]=s1    #Replace our string in file\n",
    "with open(html_repo+'box_plot_2.html', 'w') as file:\n",
    "    file.writelines(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Plot Updates\n",
    "With new readings additions/substraction happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "newL=['pH ','Temp[C]','D.O.[ppm]','Turb[FNU]', 'EC[muS per cm]','Am[mV]','Ni[ppm]','CDOM [RFU]','CHL-a [RFU]','Tryptophan [RFU]']\n",
    "\n",
    "\n",
    "s1='['\n",
    "for i in newL:\n",
    "    q=pd.Series(new_df_RJMN[i].values,index=new_df_RJMN['Date']).dropna()\n",
    "    l=str(q.to_json(orient='columns'))\n",
    "    l=l.replace('\\\"','')\n",
    "    l=l.replace(',','],[')\n",
    "    l=l.replace(':',',')\n",
    "    l=l.replace('{','[[')\n",
    "    l=l.replace('}',']]')\n",
    "    s1+=l\n",
    "    s1+=','\n",
    "    q=pd.Series(new_df_NRSP[i].values,index=new_df_NRSP['Date']).dropna()\n",
    "    l=str(q.to_json(orient='columns'))\n",
    "    l=l.replace('\\\"','')\n",
    "    l=l.replace(',','],[')\n",
    "    l=l.replace(':',',')\n",
    "    l=l.replace('{','[[')\n",
    "    l=l.replace('}',']]')\n",
    "    s1+=l\n",
    "    s1+=','\n",
    "    q=pd.Series(new_df_YM_DL[i].values,index=new_df_YM_DL['Date']).dropna()\n",
    "    l=str(q.to_json(orient='columns'))\n",
    "    l=l.replace('\\\"','')\n",
    "    l=l.replace(',','],[')\n",
    "    l=l.replace(':',',')\n",
    "    l=l.replace('{','[[')\n",
    "    l=l.replace('}',']]')\n",
    "    s1+=l\n",
    "    s1+=','\n",
    "    q=pd.Series(new_df_VRNS[i].values,index=new_df_VRNS['Date']).dropna()\n",
    "    l=str(q.to_json(orient='columns'))\n",
    "    l=l.replace('\\\"','')\n",
    "    l=l.replace(',','],[')\n",
    "    l=l.replace(':',',')\n",
    "    l=l.replace('{','[[')\n",
    "    l=l.replace('}',']]')\n",
    "    s1+=l\n",
    "    s1+=','\n",
    "s1='var dat1 = '+s1[:-1]\n",
    "s1+='];\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2='['\n",
    "for i in newL:\n",
    "    q=pd.Series(new_df_KLKT[i].values,index=new_df_KLKT['Date']).dropna()\n",
    "    l=str(q.to_json(orient='columns'))\n",
    "    l=l.replace('\\\"','')\n",
    "    l=l.replace(',','],[')\n",
    "    l=l.replace(':',',')\n",
    "    l=l.replace('{','[[')\n",
    "    l=l.replace('}',']]')\n",
    "    s2+=l\n",
    "    s2+=','\n",
    "    q=pd.Series(new_df_KNPR[i].values,index=new_df_KNPR['Date']).dropna()\n",
    "    l=str(q.to_json(orient='columns'))\n",
    "    l=l.replace('\\\"','')\n",
    "    l=l.replace(',','],[')\n",
    "    l=l.replace(':',',')\n",
    "    l=l.replace('{','[[')\n",
    "    l=l.replace('}',']]')\n",
    "    s2+=l\n",
    "    s2+=','\n",
    "    q=pd.Series(new_df_NR[i].values,index=new_df_NR['Date']).dropna()\n",
    "    l=str(q.to_json(orient='columns'))\n",
    "    l=l.replace('\\\"','')\n",
    "    l=l.replace(',','],[')\n",
    "    l=l.replace(':',',')\n",
    "    l=l.replace('{','[[')\n",
    "    l=l.replace('}',']]')\n",
    "    s2+=l\n",
    "    s2+=','\n",
    "    q=pd.Series(new_df_BGLR[i].values,index=new_df_BGLR['Date']).dropna()\n",
    "    l=str(q.to_json(orient='columns'))\n",
    "    l=l.replace('\\\"','')\n",
    "    l=l.replace(',','],[')\n",
    "    l=l.replace(':',',')\n",
    "    l=l.replace('{','[[')\n",
    "    l=l.replace('}',']]')\n",
    "    s2+=l\n",
    "    s2+=','\n",
    "s2='var dat3 = '+s2[:-1]\n",
    "s2+='];\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(html_repo+'box_plot.html', 'r') as file:\n",
    "    data = file.readlines()\n",
    "data[24]=s1    #Replace our string in file\n",
    "data[25]=s2    #Replace our string in file\n",
    "with open(html_repo+'box_plot.html', 'w') as file:\n",
    "    file.writelines(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot Updates\n",
    "\n",
    "With new readings additions/substraction happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
